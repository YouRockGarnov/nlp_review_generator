{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline INF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBVr84s-Jbjq",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "В данном ноутбуке я \"дотренировываю\" модель gpt-2 \"117M\" (это предтренированная модель с 117 миллионами параметров, которая дообучается на нашем датасете)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SD2zRyKKImb",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "! Данный ноутбук нужно запускать из Google Colab !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq_L3J3UJ6fC",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Импортируем нужные модули, gpt-2 работает с tensorflow первой версии."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hdr-6u0O4ck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "756fbe02-ccc3-4b39-af08-a18e91e34b52",
        "pycharm": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6QMw_C-KC_D",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Проверяем, что сейчас мы используем GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwSRSYpqRSqy",
        "colab_type": "code",
        "outputId": "01e3430f-b354-4d64-b889-a7131f3af8ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "pycharm": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "device_name \u003d tf.test.gpu_device_name()\n",
        "if device_name !\u003d \u0027/device:GPU:0\u0027:\n",
        "  raise SystemError(\u0027GPU device not found\u0027)\n",
        "print(\u0027Found GPU at: {}\u0027.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v4VHq5yCCL_",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import bz2\n",
        "import gc\n",
        "import chardet\n",
        "import re\n",
        "import os\n",
        "\n",
        "LINE_COUNT \u003d 100000 # количество отзывов, которое мы дадим модели, т.е. размер нашего датасета\n",
        "root_path \u003d \"/content/drive/My Drive/NLP/inf\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk_LV6U3Kc6Y",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Чтобы не потерять прогресс, добавим Google Drive как хранилище, чтобы gpt2 могло складывать туда результаты и модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77-jIlbuINO0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9efc84c1-9468-497f-8417-c7de04f4a398",
        "pycharm": {}
      },
      "source": "gpt2.mount_gdrive()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount\u003dTrue).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Скачиваем датасет, если его ещё у нас нет в Google Drive и препарируем данные - удаляем лейблы и другой мусор вроде url.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3CP6xRiwI4C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "50785ae9-31d7-48f4-96a6-389566ac8fe4",
        "pycharm": {}
      },
      "source": "file_name \u003d root_path + \u0027/prepared_data.txt\u0027\nif not os.path.exists(root_path) or not os.path.exists(file_name):\n    print(\u0027--- Making the work folder\u0027)\n\n    if not os.path.exists(\"/content/drive/My Drive/NLP\"):\n        print(\"--- Downloading the dataset\")\n\n        ! wget https://storage.googleapis.com/amazonreviews/train.ft.txt.bz2\n        ! wget https://storage.googleapis.com/amazonreviews/test.ft.txt.bz2\n        ! bzip2 -d train.ft.txt.bz2\n        ! bzip2 -d test.ft.txt.bz2\n\n        ! mkdir \"/content/drive/My Drive/NLP\"\n        ! mkdir \"$root_path\"\n\n        ! mv \"/content/train.ft.txt\" \"/content/drive/My Drive/NLP\"\n        ! mv \"/content/test.ft.txt\" \"/content/drive/My Drive/NLP\"\n\n    with open(\"/content/drive/My Drive/NLP\" + \u0027/train.ft.txt\u0027) as train_file:\n        train_file_lines \u003d train_file.readlines()\n\n    print(len(train_file_lines))\n    train_file_lines \u003d train_file_lines[:LINE_COUNT] # inf\n    print(train_file_lines[10])\n    print(len(train_file_lines))\n\n    train_labels \u003d [0 if x.split(\u0027 \u0027)[0] \u003d\u003d \u0027__label__1\u0027 else 1 for x in train_file_lines]\n    train_sentences \u003d [x.split(\u0027 \u0027, 1)[1][:-1].lower() for x in train_file_lines]\n\n    for i in range(len(train_sentences)):\n        train_sentences[i] \u003d re.sub(\u0027\\d\u0027,\u00270\u0027,train_sentences[i])\n\n                                                          \n    for i in range(len(train_sentences)):\n        if \u0027www.\u0027 in train_sentences[i] or \u0027http:\u0027 in train_sentences[i] or \u0027https:\u0027 in train_sentences[i] or \u0027.com\u0027 in train_sentences[i]:\n            train_sentences[i] \u003d re.sub(r\"([^ ]+(?\u003c\u003d\\.[a-z]{3}))\", \"\u003curl\u003e\", train_sentences[i])\n            \n\n    del train_file_lines\n\n    gc.collect()\n\n    with open(file_name, \u0027w\u0027) as prepared_file:\n      for feedback in train_sentences:\n        prepared_file.write(\"%s\\n\" % feedback)\n\n    gc.collect()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Making the work folder\n",
            "3600000\n",
            "__label__1 The Worst!: A complete waste of time. Typographical errors, poor grammar, and a totally pathetic plot add up to absolutely nothing. I\u0027m embarrassed for this author and very disappointed I actually paid for this book.\n",
            "\n",
            "100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9PPYaAjKo4y",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Загружаем модель. Я использовал самую малую \"117М\", потому что обучение на других занимает очень много времени и Google Colab нередко падает из-за нехватки RAM. Для более точных результатов советую взять версию побольше (см. тут https://github.com/openai/gpt-2/issues/209)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLjM-tVa9e8j",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "model_name \u003d \"117M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\t# gpt2.download_gpt2(model_dir\u003droot_path + \u0027/models\u0027, model_name\u003dmodel_name)   # model is saved into current directory under /models/774M/\n",
        "\tgpt2.download_gpt2(model_name\u003dmodel_name)   # model is saved into current directory under /models/774M/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otPt9YW8LCE_",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Начинаем сессию"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWxBRDtmH7wi",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "gc.collect()\n",
        "sess \u003d gpt2.start_tf_sess()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSHAoxbfLF13",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": "Тренируем модель и сохраняем результаты на Google Drive. Тут все параметры никак не влияют на качество модели (кроме steps и learning rate, но есть уже хорошие значения по умолчанию и авторы уверяют что не стоит их менять), лишь на процесс обучения (эти параметры были взяты из официальной документации). В процессе обучения модель выводит пробные тексты, можно посмотреть за её эволюцией :)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MMVpd3xPDvk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d67c31cb-c13d-43d0-f085-fb8f95035bc2",
        "pycharm": {}
      },
      "source": [
        "# gc.collect()\n",
        "gpt2.finetune(sess,\n",
        "              file_name,\n",
        "              model_name\u003dmodel_name,\n",
        "              # model_dir\u003droot_path + \u0027/models\u0027,\n",
        "              # checkpoint_dir\u003droot_path + \u0027/checkpoint\u0027,\n",
        "              max_checkpoints\u003d5,\n",
        "              print_every\u003d10,\n",
        "              save_every\u003d500,\n",
        "              sample_every\u003d200,\n",
        "              steps\u003d1000)   # steps is max number of training steps\n",
        "gpt2.copy_checkpoint_to_gdrive()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NLP/inf\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/117M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/117M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00\u003c?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:46\u003c00:00, 46.26s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 10163761 tokens\n",
            "Training...\n",
            "[10 | 28.53] loss\u003d3.71 avg\u003d3.71\n",
            "[20 | 51.05] loss\u003d3.50 avg\u003d3.61\n",
            "[30 | 74.18] loss\u003d3.60 avg\u003d3.60\n",
            "[40 | 97.68] loss\u003d3.56 avg\u003d3.59\n",
            "[50 | 120.66] loss\u003d3.45 avg\u003d3.56\n",
            "[60 | 143.62] loss\u003d3.54 avg\u003d3.56\n",
            "[70 | 166.81] loss\u003d3.51 avg\u003d3.55\n",
            "[80 | 190.03] loss\u003d3.50 avg\u003d3.54\n",
            "[90 | 213.15] loss\u003d3.60 avg\u003d3.55\n",
            "[100 | 236.26] loss\u003d3.51 avg\u003d3.55\n",
            "[110 | 259.41] loss\u003d3.51 avg\u003d3.54\n",
            "[120 | 282.63] loss\u003d3.34 avg\u003d3.52\n",
            "[130 | 305.88] loss\u003d3.46 avg\u003d3.52\n",
            "[140 | 329.08] loss\u003d3.54 avg\u003d3.52\n",
            "[150 | 352.26] loss\u003d3.65 avg\u003d3.53\n",
            "[160 | 375.44] loss\u003d3.40 avg\u003d3.52\n",
            "[170 | 398.62] loss\u003d3.56 avg\u003d3.52\n",
            "[180 | 421.78] loss\u003d3.63 avg\u003d3.53\n",
            "[190 | 444.94] loss\u003d3.61 avg\u003d3.53\n",
            "[200 | 468.09] loss\u003d3.42 avg\u003d3.53\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d SAMPLE 1 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            ".\n",
            "don\u0027t care: if you wanted to get away from this book to enjoy what everyone has been through and what you have to do to stay alive, you are going to have to waste your money. i would buy this book if i were your 00rd birthday present and never finished it. it should go back to the library where the book was supposed to be. i have just read this book, and did not like the story or the character. you will never have time to return it. i would even give it 0 stars, even if it was the least boring.\n",
            "not as exciting as the first, but still worth reading: i love the mystery of anachronos and i was very pleased that i had received my book in the mail as soon as i got it to put on. the book was good as well. i gave two stars as well, for the mystery involved in the first novel, and also for the romance in the second.\n",
            "the worst book to read: i\u0027m not sure what i was supposed to be thinking. this book has been one of the best books i\u0027ve ever read. it is a very dark, boring read, with a very dull character development. i really like the characters and what they have been brought into the story is great. i would just to add, that anachronos is one of the worst novels i\u0027ve ever enjoyed reading.\n",
            "i didn\u0027t like the first one (better one): the first time i read anachronos \"the worst\" i liked this one. i didn\u0027t like it as much as i did in the book (like when the reader goes to a restaurant for food). i didn\u0027t like the storyline or the characters from the book. i didn\u0027t like the fact that we were introduced to this world and the mystery of the mysterious man who is behind the murders that take place in this world. in the other one, i read the story in secret, because i had to write it. i found that this one wasn\u0027t quite as bad, but it definitely wasn\u0027t quite as good as the first one. still, it was good.i didn\u0027t even consider it a good book either. it didn\u0027t help my writing a lot because i was a little sad to see it fall off in the reviews.\n",
            "a terrible sequel if you like the first: i was disappointed when it came time to write this book. i had to do several things. the first thing i did was read the book. i watched as the characters were introduced in the second \"new world\" that is the mystery. it was not only the first novel in the book, but one of the few characters that you know well. i would give nothing else to the rest of this one.\n",
            "not quite as bad as the first: this is not a book that you could really read very fast. i was frustrated and wanted to just be able to find a character and enjoy it. i thought the only characters were the one i saw in the book, but i was disappointed because i wanted to be able to find that new world that the mystery surrounding. i don\u0027t think that a novel can change any of the characters. they are still alive in the book, which they are in the story. i didn\u0027t like it because it was too boring on so many levels. i couldn\u0027t enjoy it the way i felt in the first one if i enjoyed the story and had read it. it just wasn\u0027t.\n",
            "i don\u0027t like it: i think a book by a person who is famous would be the worst novel to read. at first i thought a story about a man is quite interesting and can be quite interesting and interesting, this was not the case. one of the characters and one of the main characters had to be involved in the events. when i read the second part of the book i was disappointed because i did not like it as much as i did in the first one.\n",
            "great for a novel...: i love all the different aspects of anachronos from the very first one to the next. i liked the idea of solving the story the first one. the book ends in a different way and i would think one would prefer the other.\n",
            "fantastic plot: the idea of anachronos is a great story. it is very difficult to read a novel that has only two characters and is basically a mystery.i still enjoyed the book.i think it\u0027s a very good one.\n",
            "not as exciting as the first: in the first one the characters are introduced and the plot continues to unfold. i found the story quite exciting and captivating.\n",
            "a good read: this novel is a good read but i didn\u0027t like the first one very much after that, it was a lot better later and i don\u0027t think i liked it more than once. a good read, a good story.\n",
            "awful: i gave this book a four-star rating after reading it for my 00th birthday and i liked it so much of it i thought i\n",
            "\n",
            "[210 | 502.80] loss\u003d3.38 avg\u003d3.52\n",
            "[220 | 526.08] loss\u003d3.41 avg\u003d3.52\n",
            "[230 | 549.27] loss\u003d3.40 avg\u003d3.51\n",
            "[240 | 572.43] loss\u003d3.31 avg\u003d3.50\n",
            "[250 | 595.66] loss\u003d3.50 avg\u003d3.50\n",
            "[260 | 618.94] loss\u003d3.32 avg\u003d3.49\n",
            "[270 | 642.25] loss\u003d3.59 avg\u003d3.50\n",
            "[280 | 665.56] loss\u003d3.35 avg\u003d3.49\n",
            "[290 | 688.83] loss\u003d3.62 avg\u003d3.50\n",
            "[300 | 712.07] loss\u003d3.47 avg\u003d3.49\n",
            "[310 | 735.30] loss\u003d3.42 avg\u003d3.49\n",
            "[320 | 758.53] loss\u003d3.39 avg\u003d3.49\n",
            "[330 | 781.76] loss\u003d3.44 avg\u003d3.49\n",
            "[340 | 804.98] loss\u003d3.48 avg\u003d3.49\n",
            "[350 | 828.19] loss\u003d3.48 avg\u003d3.49\n",
            "[360 | 851.36] loss\u003d3.46 avg\u003d3.49\n",
            "[370 | 874.56] loss\u003d3.48 avg\u003d3.49\n",
            "[380 | 897.80] loss\u003d3.47 avg\u003d3.49\n",
            "[390 | 921.05] loss\u003d3.44 avg\u003d3.48\n",
            "[400 | 944.31] loss\u003d3.60 avg\u003d3.49\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d SAMPLE 1 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            " fingers, i thought i\u0027d take it for a ride.\n",
            "i have this item - please not return it: i have very disappointed with this item. it was shipped defective on time. i can only assume that it will arrive damaged and i will have to return this item and send the replacement to amazon who will return it with a new item. the seller of the item and the seller of the item are of the same opinion.\n",
            "dude, you need to watch your name!): i was a sucker for the sony box-wielding movie \"dude\" from 0000-00000...it was such a fun experience. i can\u0027t stop thinking about the movie when i watch it....i\u0027m on the lookout for a box-mike, and can guarantee you that the special effects were done in high definition. that\u0027s the way i wish for the movie to be...okay, it wasn\u0027t perfect.\n",
            "so close...: i can\u0027t believe i read this. my two cats started licking all around it. the movie is just so close...it looks like it\u0027s about to hit theaters. this has a cool effect. the characters are awesome.\n",
            "do me one better!: this was a very interesting piece in that it provided me a lot of insight into the animals that live in this desert. it\u0027s like a trip to a \"world without words\" - they are not there. the only good thing that i\u0027ve seen of the story is the animals of the area. it was cool to be able to see how wild they (the animals) are.\n",
            "this may be the best sony i\u0027ve ever read!: this book is very interesting and fun to read. there\u0027s an incredible amount to it, such as how animals and humans can come to each other. this book should give you great ideas for how animals (and humans) live in their communities. i especially enjoyed the creatures of the desert. i would recommend this book to anyone from animals and human relationships, even though it may be somewhat different from your experience in the wilderness.\n",
            "hmmmmmm!!!!! a little bit of a bore...: i just love this book. i read about it about 0 times. it was easy to get to. i was able to follow the progress on any road map.\n",
            "great read!: i can\u0027t recommend this film enough! the characters are so very real. i love the animals, but that is what i love about this story. i loved the book and was hooked.\n",
            "very happy: it\u0027s a fantastic book, it\u0027s easy to understand and a good book.\n",
            "a wonderful, warm, and entertaining movie!: so thrilled to discover the animal family at the heart of the movie. it is a wonderful movie, and it is entertaining too. the movie is a must purchase for anyone who likes these animals.\n",
            "dumb and uninspired.: in the movie \"the animal\" the animals are portrayed, the movie is about the animal family of earth\u0027s first inhabitants. it is a wonderful and moving book for the ages. i was not expecting that this book would be written in 0000 or 0000 or 0000, but i did! the book is very easy to follow, you learn to look at what these wild animals look like and live. i\u0027ve read all the animals written by the author and they are all pretty great illustrations and illustrations. i thought this book was just a bit old, but it is worth reading for anyone looking for the book. i liked the book well.\n",
            "not sure what happend?: i read the book one way \u0026 the other way it was too boring. i was so embarrassed to read the book and thought i should read it another way.\n",
            "wonderful: i really enjoyed this book!! i really enjoyed animal history. it was a bit like reading \"the dog\": the animals are alive in their environment! the dogs love to eat people but when they are hungry they eat their pets! it was very touching.\n",
            "it is just great...: i started reading about five years ago and found it all a little more interesting and i haven\u0027t read about animals in literature since. and i was not too dissapointed when i found out that it was written by a 0st grader - now, i\u0027ve read it and i love it!\n",
            "an intriguing book: this book was interesting when it was first published and still is today. it will be exciting and interesting for any animal lover. my kids are older and i think that is going to be a good thing. this book is a good book for preschoolers. if you are a preschooler (as i am) it could be helpful or you will need to read it to them.\n",
            "hundreds of pages! this is not too bad for the price and is all i\u0027d wanted out of a book!\n",
            "interesting: i\u0027m a huge animal lover of horses and all horses, so when i saw this book in the mail two days ago (i was expecting it on my first trip back home) i was more excited than i\n",
            "\n",
            "[410 | 977.91] loss\u003d3.41 avg\u003d3.48\n",
            "[420 | 1001.18] loss\u003d3.55 avg\u003d3.49\n",
            "[430 | 1024.47] loss\u003d3.40 avg\u003d3.48\n",
            "[440 | 1047.74] loss\u003d3.41 avg\u003d3.48\n",
            "[450 | 1071.02] loss\u003d3.47 avg\u003d3.48\n",
            "[460 | 1094.22] loss\u003d3.52 avg\u003d3.48\n",
            "[470 | 1117.42] loss\u003d3.27 avg\u003d3.48\n",
            "[480 | 1140.62] loss\u003d3.43 avg\u003d3.48\n",
            "[490 | 1163.82] loss\u003d3.53 avg\u003d3.48\n",
            "[500 | 1187.06] loss\u003d3.48 avg\u003d3.48\n",
            "Saving checkpoint/run1/model-500\n",
            "[510 | 1213.18] loss\u003d3.50 avg\u003d3.48\n",
            "[520 | 1236.49] loss\u003d3.35 avg\u003d3.47\n",
            "[530 | 1259.68] loss\u003d3.46 avg\u003d3.47\n",
            "[540 | 1282.85] loss\u003d3.47 avg\u003d3.47\n",
            "[550 | 1306.06] loss\u003d3.46 avg\u003d3.47\n",
            "[560 | 1329.30] loss\u003d3.33 avg\u003d3.47\n",
            "[570 | 1352.59] loss\u003d3.36 avg\u003d3.47\n",
            "[580 | 1375.86] loss\u003d3.55 avg\u003d3.47\n",
            "[590 | 1399.14] loss\u003d3.44 avg\u003d3.47\n",
            "[600 | 1422.38] loss\u003d3.36 avg\u003d3.47\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d SAMPLE 1 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u0027t know how old he is but it\u0027s probably too young for my taste. i\u0027d give it 0 stars if i didn\u0027t already have a cd or cd.\n",
            "just a little over the top: i bought this cd at the age of 0, and the first thing i heard on the radio was \"it really is.\" the other minute i got a good, clear, and honest listen to my new favorite song. then i listened and found that the vocals and bass line wasn\u0027t enough. my parents bought this cd in the 00s, and it was such a disappointment. i love this cd, even going as far as to check it out on the local video library, so i won\u0027t be wasting any money on another album. i\u0027m still a fan, as i\u0027m a music junkie, and i appreciate that some folks do enjoy the music - but if they get into it, there\u0027s no point in buying anything from it. the only \"good\" thing about this purchase was that it seemed to \"give off\" a \"wow\" moment in a \"wow\" moment. i didn\u0027t think about liking new music for a while, when i was a kid, but maybe i will get over it soon.\n",
            "loved it: i like the way she speaks, her voice and style is beautiful. my opinion is that she has done very well as an actress. she has a very good voice. i like that there is no \"hilarious song\", and i think that is exactly what we want for an actress. it seems to me that the \"hilarious\" thing is a product of her ability to express herself emotionally and emotionally, with a passion. i have a lot of respect for her, and feel she has gotten better as a person.\n",
            "sounds a little dated: my opinion on this album has changed. i was expecting something different, but i was expecting something that would resonate with me. i love the title track. i love how she sings, but i don\u0027t feel that she is in charge. i just like the background vocals. i have heard the new ones, but didn\u0027t find those, so i could not say whether that is true. i would not order more from her. but i have heard them, and found them to be a very different album.\n",
            "i\u0027m glad to have listened to it!: i am glad that there was a recording of something that i could like and listen to. i am glad that there were some songs that i could listen to. i like how she sings, and she is great. i like her voice. i love how she talks. and that is another good thing.i\u0027m glad to have liked it.\n",
            "a good album but not enough for my taste: i think that sarah can call this \"a good album\", and that there are plenty of good songs about that. but she just doesn\u0027t have a good rhythm like when she sings, and she seems to be a huge fan of brian mccaffrey.\n",
            "don\u0027t watch this one: i ordered this album with the intention of buying it. when i opened it and saw the words, \"she sounds like she was on the road with her boyfriend in the 00 \u002700s and she\u0027s still with his girlfriend all the time.\" (i.e. now she\u0027s going to be home for the holidays with their child and maybe she gets to dance a bit). the only reason i bought this album was because i had heard the album before. when i went to see it, however, when i heard it, i thought it was great. when i opened the cd, it looked like it was going to be very good. however, when i bought this album, i didn\u0027t realize that i wasn\u0027t buying it for an audience, but the people who came to see it.\n",
            "a true and entertaining musical experience: a true delight for any listening audience! great music, yet not too long. sarah is great at singing, singing with passion, and is one of them. my favorite songs of all time are my sister singing the song \"drew\", and i really, really appreciate the song \"we have it all\" sung by sarah.\n",
            "beautiful love song: i really like it! it\u0027s gorgeous. love, but it doesn\u0027t have 00\u0027s style. just sweet sweet sweet, sweet. perfect!\n",
            "great!: i love the songs i listen to the most. very relaxing, happy and all kinds of. i like the songs sung by sarah and have always wanted to see her live.\n",
            "songs, but not this music!!!!!: sarah has a beautiful voice and wonderful voice. i\u0027m glad that i listened to the first two songs...it is beautiful.\n",
            "not what a good singer like sarah thought: this album does not have her voice in it. and i had no idea that i was singing \"so sad\". this album is very boring. i don\u0027t even want to hear \"hazoo\" because i never thought that\n",
            "\n",
            "[610 | 1455.79] loss\u003d3.43 avg\u003d3.47\n",
            "[620 | 1478.98] loss\u003d3.60 avg\u003d3.47\n",
            "[630 | 1502.19] loss\u003d3.38 avg\u003d3.47\n",
            "[640 | 1525.44] loss\u003d3.33 avg\u003d3.46\n",
            "[650 | 1548.71] loss\u003d3.47 avg\u003d3.46\n",
            "[660 | 1571.99] loss\u003d3.35 avg\u003d3.46\n",
            "[670 | 1595.25] loss\u003d3.27 avg\u003d3.46\n",
            "[680 | 1618.53] loss\u003d3.47 avg\u003d3.46\n",
            "[690 | 1641.78] loss\u003d3.40 avg\u003d3.46\n",
            "[700 | 1665.02] loss\u003d3.24 avg\u003d3.45\n",
            "[710 | 1688.25] loss\u003d3.57 avg\u003d3.46\n",
            "[720 | 1711.44] loss\u003d3.40 avg\u003d3.45\n",
            "[730 | 1734.62] loss\u003d3.36 avg\u003d3.45\n",
            "[740 | 1757.79] loss\u003d3.41 avg\u003d3.45\n",
            "[750 | 1780.98] loss\u003d3.39 avg\u003d3.45\n",
            "[760 | 1804.22] loss\u003d3.56 avg\u003d3.45\n",
            "[770 | 1827.47] loss\u003d3.44 avg\u003d3.45\n",
            "[780 | 1850.73] loss\u003d3.41 avg\u003d3.45\n",
            "[790 | 1874.00] loss\u003d3.37 avg\u003d3.45\n",
            "[800 | 1897.28] loss\u003d3.44 avg\u003d3.45\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d SAMPLE 1 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            " and the two are good together. i recommend this book. i can\u0027t wait to get it on my vhs. there\u0027s nothing like watching it.\n",
            "great book that could inspire people: as a parent i used this book once every year to make me feel good about myself and when i was younger it always took me a little while to get past having a child who was like that. and i loved it so much that i had to buy it from a book store!!! it makes me feel good and good to remember that everyone is here, but i don\u0027t think i\u0027m going to be able to understand for myself once my child is around us. it\u0027s so true that this should be our most cherished memory. and also, it\u0027s an antidote to the old self-defeating shame we had of not having enough self-control in childhood. this is one of the best books that comes out of the summer to make it easy for kids and adults alike to understand the meaning of a moment they\u0027re so much in control of and feel so proud of.\n",
            "if you are a young adult: this one is a must read. i enjoy the book more than many other books, but don\u0027t get me wrong. i think that this was a fun book for young children to read, especially for children ages 0 to 00. however, the one thing it didn\u0027t contain was the word \"children\".\n",
            "the first great book i have ever read.: i was surprised with the reaction i got from the parents and teachers who were all the same. it\u0027s so good and it\u0027s been a privilege to read it since i started watching tv. i loved it. it is my favorite book to watch every year and the way it shows how you want to act, and what to do, makes me smile.\n",
            "a wonderful life: i don\u0027t know exactly how much a child understands what he has been through. some children just don\u0027t relate enough to how they experience things or what they see in this world. i thought it was a very creative approach to this young adult\u0027s learning curve so i picked it up after my first day. this young adult\u0027s book and it are the only books i have ever owned to help children with the joy of reading about their lives at the end of adolescence.\n",
            "the worst books i have ever read!: this book is not even close to being a child\u0027s book! it is filled with stupid and horrible information, and is filled with a self-indulgent self hating humor, juvenile and immature humor, and juvenile and immature self-defeating humor. the author\u0027s character, the author himself, is actually very good. he would love to get that 00-000 page book but that takes him a little while, especially considering his age. his self-loathing doesn\u0027t really help much, he is just arrogant, and he is not the type of person that you would expect him to be able to relate to. his friends also don\u0027t really do anything to help him, and he is basically just a little kid, and that only makes him worse. the book takes a toll of itself and is so dull! i didn\u0027t know that he was 00 until i started watching this book, and i didn\u0027t know what i was going to find. overall, i am a huge kid who has always been a big part of my childhood.\n",
            "great book: this is one of the best books i have ever read. the character is really great. i love it.\n",
            "this book is real good.: i wish i could give it four stars for it\u0027s realism, but like most books, i really like to believe it is real. it\u0027s full of bad jokes and bad endings, and the writing is realy good. the books are made to look like a true story.\n",
            "a very good book: i read this when my 0 year old was about to read this book. i found the book as a joke. i felt that, like a kid from high school that i didn\u0027t understand, i was falling into a deep, horrible world at a very young age. i hated the author because he was writing a very juvenile book. he made me laugh out loud, and i thought it would all turn out just fine.\n",
            "a good book with some good lines and some bad lines.: i love the way it reads and the funny, juvenile moments are very funny. if you just like to laugh you will enjoy this book. it was like it was a laugh to remember. i just read it and it was my best book.\n",
            "awesome: a book that just makes me remember. i love it\n",
            "a great book, however: this is a great book for the book school age or so. the characters are amazing and the humor is so great that i have to say i will miss alot of them. i know there may be some of them i like and some of them i like. this book is so hard to understand and i didn\u0027t understand any more and all the parts that make you remember that they are so good\n",
            "\n",
            "[810 | 1930.74] loss\u003d3.47 avg\u003d3.45\n",
            "[820 | 1953.92] loss\u003d3.41 avg\u003d3.45\n",
            "[830 | 1977.06] loss\u003d3.39 avg\u003d3.45\n",
            "[840 | 2000.23] loss\u003d3.33 avg\u003d3.45\n",
            "[850 | 2023.43] loss\u003d3.36 avg\u003d3.44\n",
            "[860 | 2046.62] loss\u003d3.27 avg\u003d3.44\n",
            "[870 | 2069.79] loss\u003d3.19 avg\u003d3.44\n",
            "[880 | 2093.00] loss\u003d3.43 avg\u003d3.44\n",
            "[890 | 2116.22] loss\u003d3.44 avg\u003d3.44\n",
            "[900 | 2139.43] loss\u003d3.34 avg\u003d3.44\n",
            "[910 | 2162.68] loss\u003d3.26 avg\u003d3.43\n",
            "[920 | 2185.93] loss\u003d3.40 avg\u003d3.43\n",
            "[930 | 2209.17] loss\u003d3.50 avg\u003d3.43\n",
            "[940 | 2232.40] loss\u003d3.59 avg\u003d3.44\n",
            "[950 | 2255.57] loss\u003d3.37 avg\u003d3.43\n",
            "[960 | 2278.75] loss\u003d3.42 avg\u003d3.43\n",
            "[970 | 2301.93] loss\u003d3.39 avg\u003d3.43\n",
            "[980 | 2325.13] loss\u003d3.26 avg\u003d3.43\n",
            "[990 | 2348.33] loss\u003d3.32 avg\u003d3.43\n",
            "[1000 | 2371.59] loss\u003d3.33 avg\u003d3.43\n",
            "Saving checkpoint/run1/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBQ7I5qRL418",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Попробуем поиграться с разными параметрами генерирования текстов, например длиной."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfff7xj9L-6K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "9ba6a4a4-6ce6-498e-b07b-1855e11968f6",
        "pycharm": {}
      },
      "source": [
        "gpt2.generate(sess, length\u003d30)\n",
        "gpt2.generate(sess, length\u003d50)\n",
        "gpt2.generate(sess, length\u003d20)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"the book was the best\" - i read it, and i still love it\n",
            "the best: this is the best book i\u0027ve ever read\n",
            "cd: this is a great cd. it is great that it is not a cd, and it is not as good as the other company. i hope that this cd is okay.\n",
            "don\u0027t buy this cd: this cd is not good.\n",
            "the last fellas is a must read for any fans of the show. the last fellas is\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQzvchwjN6_x",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "О параметре top_k сами разработчики отзываются следующим образом: \n",
        "\n",
        "`\n",
        "top_k\u003d0: Integer value controlling diversity. 1 means only 1 word is considered for each step (token), resulting in deterministic completions, while 40 means 40 words are considered at each step. 0 is a special setting meaning no restrictions. 40 generally is a good value.\n",
        "`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6q9oIpYL_Zm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f7ceac81-a881-420e-9722-a123342542c3",
        "pycharm": {}
      },
      "source": [
        "gpt2.generate(sess, length\u003d30, top_k\u003d40)\n",
        "gpt2.generate(sess, length\u003d30, top_k\u003d20)\n",
        "gpt2.generate(sess, length\u003d30, top_k\u003d60)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"one of the better cds around\" the first song on my cd, it is great.i have to admit that most of the songs were\n",
            "the music is very simple and it can be enjoyed and listened to on its own. the music is very simple and you can enjoy it with any type\n",
            "\"this is a really good book. if you like it, you\u0027ll love this book. if not, i was expecting something that was good.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ulC6oPjOW4v",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Параметр temperature означает следующее:\n",
        "\n",
        "`:temperature\u003d1: Float value controlling randomness in boltzmann distribution. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.`\n",
        "\n",
        "То есть, для отзывов непохожих на отзывы из датасета нужно выбрать температуру как можно ближе к 1. Для похожих наоборот."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-COMgsKMA1g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "37116f54-a71a-4cea-dc66-f5e20d592c9a",
        "pycharm": {}
      },
      "source": [
        "gpt2.generate(sess, length\u003d30, top_k\u003d40, temperature\u003d0.1)\n",
        "gpt2.generate(sess, length\u003d30, top_k\u003d40, temperature\u003d1.0)\n",
        "gpt2.generate(sess, length\u003d30, top_k\u003d40, temperature\u003d0.4)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"the best of the best\" is a great book. i read it in the middle of the night and it was the best book i have ever\n",
            "it is simply not worth it to buy this type of product.\n",
            "it\u0027s bad.: it is simply not worth an extra 0 cents on the dollar\n",
            "\"the next generation\" is a very good book, but i have to say that it is not the best book i have ever read. i have\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv4PKE6PO6Hn",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": "Более подробный обзор на параметры gpt-2 см. в generating_baseline.ipynb, потому что данный ноутбук предназначен для обучения модели.\n"
    },
    {
      "cell_type": "markdown",
      "source": "Заключение: gpt-2 несмотря на то, что я обучил модель не полном датасете и взял самую маленькую версию модели из-за нехватки ресурсов, прекрасно справляется с генерированием различного рода текстов, в частности отзывов. Отзывы выглядят очень человечными и складными. Кроме того, данную модель можно ещё несколько раз дообучать на выборке, чтобы снизить loss, что ещё улучшит качество генерирования текстов.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ]
}